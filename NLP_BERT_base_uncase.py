# -*- coding: utf-8 -*-
"""TestingBERTBASEUncase.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EDXSEK3tBYei3RmglTbRiTwjc98YP9KA
"""

!pip install transformers

!pip install wordcloud

import os
import re
import nltk
import keras.backend as K
import random
import string
import numpy as np
import pandas as pd
import tensorflow as tf
import plotly.graph_objects as go
import plotly.express as px
import matplotlib.pyplot as plt
from nltk.probability import FreqDist
from sklearn.model_selection import train_test_split
from wordcloud import WordCloud, ImageColorGenerator ,STOPWORDS
from transformers import AutoTokenizer , TFAutoModel
from sklearn.metrics import confusion_matrix , classification_report
os.environ["WANDB_DISABLED"] = "true"

import warnings
warnings.filterwarnings('ignore')

def clean_dataset(text):
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '',text) #Removes Websites
    text  = re.sub(r'<.*?>' ,'', text)
    text = re.sub(r'\x89\S+' , ' ', text) #Removes string starting from \x89
    text = re.sub('\w*\d\w*', '', text)  # Removes numbers
    text = re.sub(r'[^\w\s]','',text)   # Removes Punctuations
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002500-\U00002BEF"  # chinese char
                               u"\U00002702-\U000027B0"
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u"\U00010000-\U0010ffff"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u200d"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\ufe0f"  # dingbats
                               u"\u3030"
                               "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)
    return text

def Convert(string):
    li = list(string.lower().split(" "))
    return li

class config:
    TRAIN_PATH = "dataset/train.csv"
    TEST_PATH = "dataset/test.csv"
    MAX_LEN = 36
    LOWER_CASE = True
    RANDOM_STATE = 12
    TEST_SIZE = 0.2
    NUM_LABELS = 1
    BATCH_SIZE = 128
    LEARNING_RATE = 5e-5
    EPOCHS = 10
    WEIGTH_DECAY = 0.01
    DEVICE = "cuda"

df_train = pd.read_csv('dataset/train.csv')
df_test = pd.read_csv('dataset/test.csv')

df_train.head(5)

random_index = random.randint(0,len(df_train)-5)
for row in df_train[["text","target"]][random_index:random_index +5].itertuples():
  _,text,target = row
  print(f"\033[2mTarget :" , f"\033[92m{target} (real disaster)" if target > 0 else f"\033[91m {target} (not a real disaster)")
  print(f"\033[114mText :\n{text}\n")
  s = clean_dataset(text)
  print(f"\033[30mCleaned Text: \n{s}\n")
  print("-------------------------------------------------------\n")

df_train["text"] = df_train["text"].map(clean_dataset)
df_train["List of Words"] = df_train["text"].map(Convert)
df_test["text"] = df_test["text"].map(clean_dataset)
df_test["List of Words"] = df_test["text"].map(Convert)

"""# Common Words"""

import nltk
nltk.download('stopwords')

stop_words = nltk.corpus.stopwords.words("english")
disaster_tweets = df_train[df_train["target"]==1]["text"].tolist()
non_disaster_tweets = df_train[df_train["target"]==0]["text"].to_list()

disaster_tweets_df = pd.DataFrame(disaster_tweets , columns = ["text"])
disaster_tweets_df["List of Words"] = disaster_tweets_df["text"].map(Convert)

non_disaster_tweets_df = pd.DataFrame(non_disaster_tweets , columns = ["text"])
non_disaster_tweets_df["List of Words"] = non_disaster_tweets_df["text"].map(Convert)

disaster_words = disaster_tweets_df["List of Words"]
disaster_allwords = []
for wordlist in disaster_words:
    for disaster_word in wordlist:
        if disaster_word not in stop_words:
            disaster_allwords.append(disaster_word)


non_disaster_words = non_disaster_tweets_df["List of Words"]
non_disaster_allwords = []
for wordlist in non_disaster_words:
    for non_disaster_word in wordlist:
        if non_disaster_word not in stop_words:
            non_disaster_allwords.append(non_disaster_word)

mostcommon = FreqDist(disaster_allwords).most_common(2000)
wordcloud = WordCloud(width=1800, height=1000, background_color='white').generate(str(mostcommon))
fig = plt.figure(figsize=(20,5), facecolor='white')
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title('Disaster Tweets common words', fontsize=40)
plt.tight_layout(pad=0)
plt.show()

mostcommon = FreqDist(non_disaster_allwords).most_common(2000)
wordcloud = WordCloud(width=1800, height=1000, background_color='white').generate(str(mostcommon))
fig = plt.figure(figsize=(20,5), facecolor='white')
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.title('Non Disaster Tweets commmon words', fontsize=40)
plt.tight_layout(pad=0)
plt.show()

test_ids = df_test["id"]
df_train = df_train.drop(["id" , "keyword" , "location" , "List of Words"] , axis = 1)
df_test = df_test.drop(["id" , "keyword" , "location" , "List of Words"], axis =1 )

"""# Model

Tokenization
"""

K.clear_session()
MODEL_1 = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_1 , do_lower_case = config.LOWER_CASE , max_length = config.MAX_LEN )
x_train = tokenizer(
        text = df_train["text"].tolist(),
        add_special_tokens = True,
        max_length = config.MAX_LEN,
        truncation = True,
        padding = True,
        return_tensors = "tf",
        return_token_type_ids = False,
        return_attention_mask = True,
        verbose = True
        )

x_test = tokenizer(
        text = df_test["text"].tolist(),
        add_special_tokens = True,
        max_length = config.MAX_LEN,
        truncation = True,
        padding = True,
        return_tensors = "tf",
        return_token_type_ids = False,
        return_attention_mask = True,
        verbose = True
        )

bert_based_uncased = TFAutoModel.from_pretrained(MODEL_1)

input_ids = tf.keras.layers.Input(shape = (config.MAX_LEN,) , dtype = tf.int32 , name = "input_ids")
input_mask = tf.keras.layers.Input(shape = (config.MAX_LEN,) , dtype = tf.int32 , name = "attention_mask")
embeddings = bert_based_uncased(input_ids , attention_mask = input_mask)[1]
x = tf.keras.layers.Dropout(0.3)(embeddings)
x = tf.keras.layers.Dense(128 , activation = "relu")(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(32 , activation = "relu")(x)
output = tf.keras.layers.Dense(config.NUM_LABELS , activation = "sigmoid")(x)

model_1 = tf.keras.Model(inputs = [input_ids , input_mask] , outputs = output)

if  os.path.isdir("./weights/bert_base_uncased_weights") is None:
          os.makedirs("./weights/bert_base_uncased_weights")
checkpoint_filepath_bert_base_uncased  = "./weights/bert_base_uncased_weights"
checkpoint_callback_bert_base_uncased = tf.keras.callbacks.ModelCheckpoint(
    checkpoint_filepath_bert_base_uncased,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='auto',
    save_best_only=True)

model_1.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),
             optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = config.LEARNING_RATE , epsilon = 1e-8 , decay  =config.WEIGTH_DECAY , clipnorm = 1.0),
             metrics = ["accuracy"])

bert_based_uncased_history  = model_1.fit(x = {"input_ids": x_train["input_ids"] , "attention_mask" : x_train["attention_mask"]},
                y = df_train["target"] ,
                epochs = config.EPOCHS ,
                validation_split = 0.2,
                batch_size = 256 , callbacks = [checkpoint_callback_bert_base_uncased])

# Assuming 'model' is your trained TensorFlow model
model_1.save('/content/drive/MyDrive/DataMiningModel_Test3')

import os
saved_model_path = '/content/drive/MyDrive/DataMiningModel_Test3'
os.listdir(saved_model_path)

#loaded_model = tf.keras.models.load_model(saved_model_path)

model_1.load_weights(checkpoint_filepath_bert_base_uncased)

bert_based_uncased_hist_df = pd.DataFrame(bert_based_uncased_history.history , columns = ['loss', 'accuracy', 'val_loss', 'val_accuracy'])

"""Graph"""

fig = plt.figure(figsize = (5,4))
plt.plot(np.arange(len(bert_based_uncased_hist_df["accuracy"]))+1,bert_based_uncased_hist_df["accuracy"],'r-',np.arange(len(bert_based_uncased_hist_df["val_accuracy"]))+1,bert_based_uncased_hist_df["val_accuracy"],'b-',linewidth=2)
plt.legend(["Accuracy" , "Validation Accuracy"])
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
#plt.title("BERT-base uncased Accuracy Plot")

fig = plt.figure(figsize = (5,4))
plt.plot(np.arange(len(bert_based_uncased_hist_df["loss"]))+1,bert_based_uncased_hist_df["loss"],'r-',np.arange(len(bert_based_uncased_hist_df["val_loss"]))+1,bert_based_uncased_hist_df["val_loss"],'b-',linewidth=2)
plt.legend(["Loss" , "Validation Loss"])
plt.xlabel("Epochs")
plt.ylabel("Loss")
#plt.title("BERT-base uncased Loss Plot")

"""# Confusion Matrix"""

y_pred = model_1.predict({"input_ids" : x_train["input_ids"] ,"attention_mask" : x_train["attention_mask"]})
y_pred = np.where(y_pred > 0.5 , 1, 0)
y_test = df_train["target"]
CLASS_LABELS = ["Disaster Tweet" , "Non Disaster Tweet"]
cm_data = confusion_matrix(y_test , y_pred)
cm = pd.DataFrame(cm_data , columns = CLASS_LABELS , index = CLASS_LABELS)
fig = px.imshow(img = cm_data ,
                x = CLASS_LABELS,
                y = CLASS_LABELS,
                aspect="auto" ,
                color_continuous_scale = "Sunset")
fig.update_xaxes(title="Predicted")
fig.update_yaxes(title = "Actual")
fig.update_layout(title = "Confusion Matrix",
                  template = "plotly_white",
                  title_x = 0.5)
fig.update_layout(
    yaxis_tickangle=-90  # Set the desired rotation angle (e.g., -45 degrees)
)
fig.show()

"""Prediction"""

model_1_pred_probs = model_1.predict({"input_ids" : x_test["input_ids"] ,"attention_mask" : x_test["attention_mask"]})
y_pred_1 = np.where(model_1_pred_probs > 0.5 , 1,0)

bert_base_uncased_df=pd.DataFrame()
bert_base_uncased_df['id'] = test_ids
bert_base_uncased_df['target'] = y_pred_1

bert_base_uncased_df.to_csv('bert_base_uncased.csv',index = False)
bert_base_uncased_df["target"].value_counts()

result = {}
result['id'] = list(test_ids)
result['target'] = list(np.squeeze(y_pred_1))
print (result)
pdObj = pd.DataFrame.from_dict(result)
pdObj.to_csv("submission_bert.csv",index=False)

